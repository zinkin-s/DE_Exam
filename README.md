

# ETL-процесс для диагностики рака груди с использованием Google Cloud Storage

## Описание проекта

Этот проект реализует автоматизированный ETL-процесс для диагностики рака груди с использованием датасета Breast Cancer Wisconsin Diagnostic. Процесс включает в себя:

- **Загрузка данных из Google Cloud Storage**
- **Очистка и предобработка данных** (удаление дубликатов, обработка пропущенных значений, нормализация)
- **Обучение модели машинного обучения** (логистическая регрессия)
- **Оценка модели** по метрикам (Accuracy, Precision, Recall, F1)
- **Сохранение результатов**
- **Выгрузка результатов** в Google Cloud Storage

Процесс оркестрован через Apache Airflow, что обеспечивает надежность, отслеживаемость и возможность расширения.

## Задача машинного обучения

Мы решаем задачу бинарной классификации: определение, является ли опухоль злокачественной (M) или доброкачественной (B) на основе характеристик клеток, извлеченных из изображения опухоли.

Используем датасет Breast Cancer Wisconsin Diagnostic, который содержит информацию о 569 пациентах с диагностированным раком груди. Данные включают 30 числовых признаков, вычисленных на основе изображений клеток, таких как радиус, текстура, периметр и другие характеристики.

## Архитектура проекта

```
breast_cancer_pipeline/
│
├── etl/                      # ETL-компоненты
│   ├── data_loader.py       # Загрузка данных из Google Cloud Storage
│   ├── data_preprocessor.py   # Очистка и предобработка данных
│   ├── model_trainer.py      # Обучение модели и оценка
│   └── cloud_uploader.py     # Выгрузка результатов в Google Cloud Storage
│
├── dags/                     # DAG-файлы для Airflow
│   └── pipeline_dag.py       # Основной DAG пайплайна
│
├── data/                     # Директория для временного хранения данных
│   ├── raw/                  # Сырые данные после загрузки из облака
│   └── processed/            # Обработанные данные
│
├── models/                   # Сохраненные модели
│
├── metrics/                  # Сохраненные метрики
│
├── logs/                     # Логи выполнения задач
│
├── config/                   # Конфигурационные файлы
│   └── gcp_credentials.json  # Учетные данные Google Cloud
│
├── requirements.txt         # Зависимости проекта
└── README.md                # Документация
```

## Схема пайплайна

```
start --> download_data --> preprocess_data --> train_model --> evaluate_model --> save_model --> upload_model --> upload_metrics --> end
```

## Технические характеристики

### Модель машинного обучения
- **Алгоритм**: Логистическая регрессия
- **Целевая переменная**: диагноз (M - злокачественный, B - доброкачественный)
- **Метрики**: Accuracy, Precision, Recall, F1
- **Нормализация**: StandardScaler

### Google Cloud Storage
- **Методы**: OAuth2 аутентификация с использованием service account, загрузка и скачивание файлов
- **Контейнеры**:
  - `raw_data_bucket` - для хранения сырых данных
  - `processed_data_bucket` - для хранения обработанных данных
  - `model_output_bucket` - для хранения обученной модели
  - `metrics_output_bucket` - для хранения метрик модели

### Apache Airflow
- **Типы операторов**: PythonOperator, DummyOperator
- **Повторные попытки**: 3
- **Таймаут задачи**: 60 минут
- **Логирование**: подробное логирование в файлы
- **Передача данных**: XCom для передачи метрик и информации между задачами

### Обработка ошибок и устойчивость

Проект устойчив к следующим потенциальным сбоям:

1. **Загрузка данных из Google Cloud**
   - Проверка наличия файла в облаке
   - Повторная аутентификация при ошибке
   - Логирование ошибок загрузки
   - Автоматическое восстановление после потери соединения

2. **Очистка и предобработка данных**
   - Проверка наличия необходимых колонок
   - Обработка пропущенных значений
   - Удаление дубликатов
   - Нормализация данных

3. **Обучение модели**
   - Проверка корректности обучающих данных
   - Обработка ошибок обучения
   - Обработка ошибок оценки
   - Проверка сохранения модели

4. **Выгрузка в Google Cloud**
   - Повторная аутентификация
   - Повторная попытка загрузки
   - Проверка контрольной суммы файла
   - Логирование ошибок загрузки

5. **Устойчивость Airflow**
   - Повторные попытки выполнения задач
   - Таймауты задач
   - Подробное логирование
   - Изоляция задач

## Установка и запуск

### Требования
- Python 3.7 или выше
- pip
- Apache Airflow
- Google Cloud Platform (GCP) аккаунт с включенным Google Cloud Storage API
- Google Cloud credentials JSON файл

### Установка

1. Клонируйте репозиторий:
   ```bash
   git clone https://github.com/your-username/breast-cancer-pipeline.git
   cd breast-cancer-pipeline
   ```

2. Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```

3. Настройте Apache Airflow:
   ```bash
   airflow db init
   airflow users create -u admin -p admin -r Admin -e admin@example.com -f Admin -l User
   ```

4. Скопируйте файл с учетными данными Google Cloud в `config/gcp_credentials.json`.

5. Настройте проект в Google Cloud Console:
   - Создайте 4 бакета в Google Cloud Storage:
     - `raw_data_bucket`
     - `processed_data_bucket`
     - `model_output_bucket`
     - `metrics_output_bucket`
   - Назначьте права доступа service account к бакетам

6. Запустите webserver и scheduler Airflow:
   ```bash
   airflow webserver --port 8080
   airflow scheduler
   ```

7. Активируйте DAG `breast_cancer_pipeline` через веб-интерфейс Airflow.

### Использование

1. Убедитесь, что файл `wdbc.csv` находится в бакете `raw_data_bucket` в Google Cloud Storage.

2. Откройте веб-интерфейс Airflow по адресу `http://localhost:8080` и проверьте статус DAG `breast_cancer_pipeline`.

3. DAG будет запускаться автоматически в соответствии с расписанием (по умолчанию ежедневно). Для ручного запуска нажмите на кнопку "Trigger DAG".

4. После успешного выполнения DAG, результаты будут доступны:
   - Обученная модель: в бакете `model_output_bucket`
   - Метрики: в бакете `metrics_output_bucket`

## Конфигурация

Проект можно настроить через следующие переменные окружения:

- `GCP_CREDENTIALS_PATH` - путь к файлу учетных данных Google Cloud (по умолчанию: `config/gcp_credentials.json`)
- `RAW_DATA_BUCKET` - имя бакета с сырыми данными (по умолчанию: `raw_data_bucket`)
- `PROCESSED_DATA_BUCKET` - имя бакета с обработанными данными (по умолчанию: `processed_data_bucket`)
- `MODEL_OUTPUT_BUCKET` - имя бакета с моделью (по умолчанию: `model_output_bucket`)
- `METRICS_OUTPUT_BUCKET` - имя бакета с метриками (по умолчанию: `metrics_output_bucket`)
- `AIRFLOW_HOME` - путь к домашней директории Airflow (по умолчанию: `/usr/local/airflow`)

Пример настройки:
```bash
export GCP_CREDENTIALS_PATH=./config/gcp_credentials.json
export RAW_DATA_BUCKET=my_raw_data_bucket
export PROCESSED_DATA_BUCKET=my_processed_data_bucket
export MODEL_OUTPUT_BUCKET=my_model_output_bucket
export METRICS_OUTPUT_BUCKET=my_metrics_output_bucket
export AIRFLOW_HOME=./airflow
```

## Возможности расширения

1. **Модели машинного обучения**
   - Добавить другие модели (Random Forest, SVM, Neural Networks)
   - Добавить кросс-валидацию
   - Добавить подбор гиперпараметров (GridSearchCV)

2. **Обработка данных**
   - Добавить валидацию данных
   - Добавить визуализацию данных
   - Добавить обнаружение аномалий

3. **Google Cloud Storage**
   - Добавить шифрование данных
   - Добавить управление версиями файлов
   - Добавить политики жизненного цикла

4. **Мониторинг и уведомления**
   - Добавить веб-дасборды
   - Добавить уведомления через Slack, Telegram, SMS
   - Добавить автоматическое отключение пайплайна

5. **Оптимизация производительности**
   - Добавить параллелизм
   - Добавить распределенные вычисления
   - Добавить кэширование промежуточных результатов

6. **Безопасность**
   - Добавить шифрование секретов
   - Добавить управление доступом
   - Добавить аудит операций

7. **Документация**
   - Добавить автоматическую генерацию документации
   - Добавить примеры использования
   - Добавить докеризацию проекта

8. **CI/CD**
   - Добавить автоматическое тестирование
   - Добавить автоматическое развертывание
   - Добавить мониторинг и оповещения

## Анализ ошибок и устойчивости

### Потенциальные точки сбоя

1. **Загрузка данных из Google Cloud**
   - Проблема: Нет подключения к Google Cloud или ошибка аутентификации
   - Решение: Увеличить время ожидания, реализовать повторные попытки подключения, сохранить текущее состояние и продолжить после восстановления подключения

2. **Невалидные данные**
   - Проблема: Некорректные данные в Google Cloud (неверный формат, отсутствуют необходимые колонки)
   - Решение: Реализовать валидацию данных перед обработкой, добавить обработку исключений, сохранить информацию об ошибке и уведомить администратора

3. **Ошибка предобработки данных**
   - Проблема: Ошибка нормализации, ошибка удаления дубликатов или обработки пропущенных значений
   - Решение: Добавить проверки на каждом этапе предобработки, реализовать обработку исключений, сохранить текущее состояние и продолжить после исправления ошибки

4. **Ошибка обучения модели**
   - Проблема: Ошибка обучения модели, модель не сходится
   - Решение: Добавить обработку исключений, реализовать повторные попытки с другими параметрами, сохранить текущее состояние и логи для анализа

5. **Ошибка выгрузки в Google Cloud**
   - Проблема: Ошибка загрузки модели или метрик в Google Cloud
   - Решение: Реализовать повторные попытки загрузки, сохранить данные локально и продолжить попытки загрузки, уведомить администратора при продолжительной ошибке

### Реализованные меры устойчивости

1. **Логирование**
   - Подробное логирование на каждом этапе
   - Логи сохраняются в `logs/` для анализа
   - Логирование времени выполнения задачи

2. **Обработка исключений**
   - Все возможные исключения обрабатываются
   - Каждая задача возвращает True/False в зависимости от успеха
   - Обработка ошибок на каждом этапе

3. **Повторные попытки**
   - Реализованы повторные попытки в Airflow (3 попытки)
   - Интервал между попытками: 5 минут
   - Настройка времени ожидания задачи: 60 минут

4. **Изоляция задач**
   - Каждая задача изолирована
   - Сбой одной задачи не влияет на другие
   - Возможность запуска отдельных задач независимо

5. **Передача данных**
   - Передача данных между задачами через XCom
   - Сохранение информации о выполненных задачах
   - Возможность возобновления процесса с любого этапа

6. **Проверка данных**
   - Проверка наличия необходимых колонок
   - Проверка корректности данных
   - Проверка наличия обучающих данных

7. **Управление состоянием**
   - Сохранение текущего состояния
   - Возможность возобновления процесса после сбоя
   - Логирование текущего состояния и прогресса

### Обработка конкретных сценариев

1. **Потеря соединения с источником данных**
   - Реализованы повторные попытки подключения
   - Сохранение текущего состояния
   - Возможность продолжения после восстановления соединения

2. **Невалидные данные**
   - Реализована валидация данных
   - Сохранение информации об ошибке
   - Уведомление администратора

3. **Ошибка обучения модели**
   - Реализована обработка исключений
   - Сохранение логов для анализа
   - Возможность повторного обучения с другими параметрами

4. **Выгрузка в Google Cloud**
   - Реализованы повторные попытки загрузки
   - Сохранение данных локально
   - Возможность продолжения загрузки после восстановления соединения

Этот проект демонстрирует высокий уровень устойчивости и надежности, что критично для медицинских систем, где сбои могут привести к серьезным последствиям.